{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import data_loader\n",
    "import losses\n",
    "import model\n",
    "import cv2\n",
    "\n",
    "from stats_func import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\\\data\\\\ct\\\\ct_train\\\\image\\\\ct_train_1001_image.nii\\\\z\\\\43.png', '\\\\data\\\\ct\\\\ct_train\\\\image\\\\ct_train_1001_image.nii\\\\z\\\\44.png', '\\\\data\\\\ct\\\\ct_train\\\\image\\\\ct_train_1001_image.nii\\\\z\\\\45.png', '\\\\data\\\\ct\\\\ct_train\\\\image\\\\ct_train_1001_image.nii\\\\z\\\\46.png', '\\\\data\\\\ct\\\\ct_train\\\\image\\\\ct_train_1001_image.nii\\\\z\\\\47.pn']\n",
      "(256, 256, 1)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 4)\n",
      "(256, 256, 4)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.train' has no attribute 'shuffle_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17812/1487031536.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./output'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig_filename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./configs/exp_01.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkfold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17812/1487031536.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(log_dir, config_filename, checkpoint_dir, skip, kfold)\u001b[0m\n\u001b[0;32m     90\u001b[0m                               checkpoint_dir, do_flipping, skip, kfold)\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0mpch_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17812/1487031536.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mTVal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_target_val_pth\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kfold\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'.txt'\u001b[0m        \u001b[1;31m# 训练时目标域图片名称\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_source_train_pth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# 调用data_load文件\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_source_val_pth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTVal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# 验证集文件（源域，目标域）（均为txt）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\paper\\DDFSeg-main\\my\\data_loader.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(source_pth, target_pth, do_shuffle)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m# Batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdo_shuffle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mimages_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages_j\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimage_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_j\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mimages_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages_j\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimage_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_j\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcapacity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.train' has no attribute 'shuffle_batch'"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "source_train_pth = './road/source_train.txt'\n",
    "target_train_pth = './road/target_train_'\n",
    "source_val_pth = './road/source_val.txt'\n",
    "target_val_pth = './road/target_val_'\n",
    "\n",
    "\n",
    "evaluation_interval = 10\n",
    "save_interval = 300\n",
    "num_cls = 4\n",
    "keep_rate_value=0.75\n",
    "is_training_value=True\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "class PCH:\n",
    "\n",
    "    def __init__(self, pool_size, lambda_a,\n",
    "                 lambda_b, output_root_dir, to_restore,\n",
    "                 base_lr, max_step,\n",
    "                 checkpoint_dir, do_flipping, skip, kfold):\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "        self._source_train_pth = source_train_pth\n",
    "        self._target_train_pth = target_train_pth\n",
    "        self._source_val_pth = source_val_pth\n",
    "        self._target_val_pth = target_val_pth\n",
    "        self._num_cls = num_cls\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "        self.keep_rate = tf.compat.v1.placeholder(tf.float32, shape=())\n",
    "        self.is_training = tf.compat.v1.placeholder(tf.bool, shape=())\n",
    "        self._pool_size = pool_size\n",
    "        self._size_before_crop = 256\n",
    "        self._lambda_a = lambda_a\n",
    "        self._lambda_b = lambda_b\n",
    "        self._output_dir = os.path.join(output_root_dir, str(kfold))\n",
    "        self._images_dir = os.path.join(self._output_dir, 'imgs')\n",
    "        self._num_imgs_to_save = 20\n",
    "        self._to_restore = to_restore\n",
    "        self._base_lr = base_lr\n",
    "        self._max_step = max_step\n",
    "        self._checkpoint_dir = checkpoint_dir\n",
    "        self._do_flipping = do_flipping\n",
    "        self._skip = skip\n",
    "        self._kfold = kfold\n",
    "\n",
    "        self.fake_images_A = np.zeros(\n",
    "            (self._pool_size, BATCH_SIZE, model.IMG_HEIGHT, model.IMG_WIDTH, 1)\n",
    "        )\n",
    "        self.fake_images_B = np.zeros(\n",
    "            (self._pool_size, BATCH_SIZE, model.IMG_HEIGHT, model.IMG_WIDTH, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # 模型训练\n",
    "\n",
    "        # 从数据集文件夹加载数据集\n",
    "        TTrain = self._target_train_pth + str(self._kfold) +'.txt'     # 训练源域图片名称\n",
    "        TVal = self._target_val_pth + str(self._kfold) +'.txt'        # 训练时目标域图片名称\n",
    "\n",
    "        self.inputs = data_loader.load_data(self._source_train_pth, TTrain, True)   # 调用data_load文件\n",
    "        self.inputs_val = data_loader.load_data(self._source_val_pth, TVal, True)   # 验证集文件（源域，目标域）（均为txt）\n",
    "\n",
    "        \n",
    "\n",
    "def main(log_dir, config_filename, checkpoint_dir, skip, kfold):\n",
    "\n",
    "    if not os.path.isdir(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    with open(config_filename) as config_file:\n",
    "        config = json.load(config_file)\n",
    "\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    # 定义超参数\n",
    "    lambda_a = float(config['_LAMBDA_A']) if '_LAMBDA_A' in config else 10.0\n",
    "    lambda_b = float(config['_LAMBDA_B']) if '_LAMBDA_B' in config else 10.0\n",
    "    pool_size = int(config['pool_size']) if 'pool_size' in config else 50\n",
    "\n",
    "    to_restore = False\n",
    "    base_lr = float(config['base_lr']) if 'base_lr' in config else 0.0002\n",
    "    max_step = int(config['max_step']) if 'max_step' in config else 100\n",
    "    do_flipping = bool(config['do_flipping'])\n",
    "\n",
    "    # 设置pch的init\n",
    "    pch_model = PCH(pool_size, lambda_a, lambda_b, log_dir,\n",
    "                              to_restore, base_lr, max_step,\n",
    "                              checkpoint_dir, do_flipping, skip, kfold)\n",
    "\n",
    "    pch_model.train()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(log_dir='./output', config_filename='./configs/exp_01.json', checkpoint_dir='', skip=True, kfold=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
